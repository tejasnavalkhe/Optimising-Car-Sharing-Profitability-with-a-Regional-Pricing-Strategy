{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOl5/Xg5UcU8FopraYzQi8T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Objectives:\n",
        "1. **Analyse Usage and Demand Patterns:** Examine the extensive trip data available in Co-Wheels’ booking system (TripIQ) to identify patterns in vehicle usage and demand across different locations and times.\n",
        "2. **Design and Develop a Pricing Model and Tool:**\n",
        "     1. Create a pricing model that incorporates fixed and variable costs, including fuel and electricity, to determine optimal hourly and daily rates for different locations and times.\n",
        "     2. Develop a straightforward tool that allows Co-Wheels to input various cost factors and receive tailored pricing options based on location, demand, and seasonal variations.   \n",
        "4. **Evaluate Seasonal and Temporal Variations:** Assess the impact of seasonal changes and time-of-day variations on car-sharing demand and integrate these factors into the pricing model.\n",
        "5. **Assess Profitability and Utilisation Impact:** Model potential outcomes of different pricing strategies to evaluate their impact on profitability and vehicle utilisation rates in various locations.\n",
        "6. **Validate pricing tool:** Test the pricing tool with real-world data to ensure its accuracy and effectiveness in optimising Co-Wheels’ pricing strategy."
      ],
      "metadata": {
        "id": "55N0hQI4J8MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Loading libraries and data"
      ],
      "metadata": {
        "id": "QN84niRvjh2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZw_rg9JZhel",
        "outputId": "4ea73777-7ba8-40c9-aef1-a42d629a263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBWRqFJgjERr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "79790a77-eed7-4c7f-f5cf-3e8671a643b1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-42b05f58ea69>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Data location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from category_encoders import BinaryEncoder, OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "import joblib\n",
        "\n",
        "# Display all the columns of the dataframe\n",
        "pd.pandas.set_option('display.max_columns', None)\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Data location\n",
        "DATA_PATH = \"/content/drive/MyDrive/MSc Dissertation/data/\"\n",
        "# Encoders location\n",
        "ENCODERS_PATH = \"/content/drive/MyDrive/MSc Dissertation/encoders/\"\n",
        "# Model location\n",
        "MODEL_PATH = \"/content/drive/MyDrive/MSc Dissertation/models/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Load data"
      ],
      "metadata": {
        "id": "wnCtbjkuod1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed = pd.read_csv(DATA_PATH + 'transformed_dataset.csv')\n",
        "df = pd.read_csv(DATA_PATH + 'scaled_dataset.csv')\n",
        "\n",
        "print('Scaled data: ', df.shape)\n",
        "print('Transformed data: ', df_transformed.shape)"
      ],
      "metadata": {
        "id": "fCa5WGurogsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Hiq--3OJ3lR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.head()"
      ],
      "metadata": {
        "id": "_hqtdrr2MDrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas datetime object\n",
        "df['booking_billed_start'] = pd.to_datetime(df['booking_billed_start'])\n",
        "df_transformed['booking_billed_start'] = pd.to_datetime(df_transformed['booking_billed_start'])\n",
        "\n",
        "# Sort data by booking_actual_start to ensure temporal order\n",
        "df = df.sort_values(by='booking_billed_start').reset_index(drop=True)\n",
        "df_transformed = df_transformed.sort_values(by='booking_billed_start').reset_index(drop=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=['vehicle_description', 'vehicle_registration', 'booking_actual_start',\n",
        "                 'booking_actual_end', 'booking_billed_end', 'booking_created_at'], inplace=True)\n",
        "\n",
        "df.shape"
      ],
      "metadata": {
        "id": "WnU05jgGMxme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "SguuM_5IOg60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.head()"
      ],
      "metadata": {
        "id": "IRClo2zXM4sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Load Encoders"
      ],
      "metadata": {
        "id": "I86Ae-n83oqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the encoders\n",
        "binary_encoder = joblib.load(ENCODERS_PATH + 'binary_encoder.pkl')\n",
        "one_hot_encoder = joblib.load(ENCODERS_PATH + 'one_hot_encoder.pkl')"
      ],
      "metadata": {
        "id": "7q7Vp7Te5uz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Developing Code"
      ],
      "metadata": {
        "id": "LBJRI72goTwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to create and compile neural network model\n",
        "def create_nn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model"
      ],
      "metadata": {
        "id": "YiPSeYT6oXXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate models\n",
        "def train_evaluate_models(df):\n",
        "    vehicle_types = [col for col in df.columns if col.startswith('Vehicle Type_')]\n",
        "    results = {}\n",
        "    predictions_df = pd.DataFrame()\n",
        "\n",
        "    for vehicle_type in vehicle_types:\n",
        "        vehicle_data = df[df[vehicle_type] == 1].copy()\n",
        "\n",
        "        # Sort the data based on time\n",
        "        vehicle_data = vehicle_data.sort_values(by='booking_billed_start')\n",
        "\n",
        "        # Define features and target for the current vehicle type\n",
        "        features_vehicle = vehicle_data.drop(columns=['hourly_rate', 'daily_rate', 'booking_billed_start', 'booking_id'])\n",
        "\n",
        "        target_hourly_vehicle = vehicle_data['hourly_rate']\n",
        "        target_daily_vehicle = vehicle_data['daily_rate']\n",
        "\n",
        "        # Split the data based on time\n",
        "        split_ratio = 0.8\n",
        "        split_index_vehicle = int(len(vehicle_data) * split_ratio)\n",
        "\n",
        "        X_train = features_vehicle.iloc[:split_index_vehicle]\n",
        "        X_test = features_vehicle.iloc[split_index_vehicle:]\n",
        "        X_test_copy = vehicle_data.drop(columns=['hourly_rate', 'daily_rate']).iloc[split_index_vehicle:]\n",
        "        y_train_hourly = target_hourly_vehicle.iloc[:split_index_vehicle]\n",
        "        y_test_hourly = target_hourly_vehicle.iloc[split_index_vehicle:]\n",
        "        y_train_daily = target_daily_vehicle.iloc[:split_index_vehicle]\n",
        "        y_test_daily = target_daily_vehicle.iloc[split_index_vehicle:]\n",
        "\n",
        "        # Set values to None\n",
        "        hourly_mae = None\n",
        "        hourly_rmse = None\n",
        "        daily_mae = None\n",
        "        daily_rmse = None\n",
        "        y_pred_hourly = None\n",
        "        y_pred_daily = None\n",
        "\n",
        "\n",
        "        if 'City' in vehicle_type or '7 Seater' in vehicle_type:\n",
        "            # Normalize the features\n",
        "            scaler = MinMaxScaler()\n",
        "            X_train = scaler.fit_transform(X_train)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Save the scaler\n",
        "            joblib.dump(scaler, ENCODERS_PATH + f'scaler_{vehicle_type}.pkl')\n",
        "\n",
        "            # Neural Network for both hourly and daily rates\n",
        "            model_hourly = create_nn_model(X_train.shape[1])\n",
        "            model_hourly.fit(X_train, y_train_hourly, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "            model_hourly.save(MODEL_PATH + f'{vehicle_type.replace(\"Vehicle Type_\", \"\")}_nn_hourly_rate_model.keras')\n",
        "\n",
        "            # Evaluate the hourly rate model\n",
        "            y_pred_hourly = model_hourly.predict(X_test)\n",
        "            hourly_mae = np.round(mean_absolute_error(y_test_hourly, y_pred_hourly), 5)\n",
        "            hourly_rmse = np.round(np.sqrt(mean_squared_error(y_test_hourly, y_pred_hourly)), 5)\n",
        "\n",
        "            model_daily = create_nn_model(X_train.shape[1])\n",
        "            model_daily.fit(X_train, y_train_daily, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "            model_daily.save(MODEL_PATH + f'{vehicle_type.replace(\"Vehicle Type_\", \"\")}_nn_daily_rate_model.keras')\n",
        "\n",
        "            # Evaluate the daily rate model\n",
        "            y_pred_daily = model_daily.predict(X_test)\n",
        "            daily_mae = np.round(mean_absolute_error(y_test_daily, y_pred_daily), 5)\n",
        "            daily_rmse = np.round(np.sqrt(mean_squared_error(y_test_daily, y_pred_daily)), 5)\n",
        "\n",
        "            print(f\"Vehicle Type: {vehicle_type.replace('Vehicle Type_', '')}\")\n",
        "            print(\"Hourly Rate Model - MAE:\", hourly_mae)\n",
        "            print(\"Hourly Rate Model - RMSE:\", hourly_rmse)\n",
        "            print(\"Daily Rate Model - MAE:\", daily_mae)\n",
        "            print(\"Daily Rate Model - RMSE:\", daily_rmse)\n",
        "            print(\"\\n\")\n",
        "\n",
        "        elif 'Everyday' in vehicle_type or 'Van' in vehicle_type:\n",
        "            # XGBRegressor for both hourly and daily rates\n",
        "            model_hourly = XGBRegressor()\n",
        "            model_hourly.fit(X_train, y_train_hourly)\n",
        "            joblib.dump(model_hourly, MODEL_PATH + f'{vehicle_type.replace(\"Vehicle Type_\", \"\")}_xgb_hourly_rate_model.pkl')\n",
        "\n",
        "            model_daily = XGBRegressor()\n",
        "            model_daily.fit(X_train, y_train_daily)\n",
        "            joblib.dump(model_daily, MODEL_PATH + f'{vehicle_type.replace(\"Vehicle Type_\", \"\")}_xgb_daily_rate_model.pkl')\n",
        "\n",
        "            # Evaluate models\n",
        "            y_pred_hourly = model_hourly.predict(X_test)\n",
        "            y_pred_daily = model_daily.predict(X_test)\n",
        "\n",
        "            # hourly and daily mae and rmse\n",
        "            hourly_mae = np.round(mean_absolute_error(y_test_hourly, y_pred_hourly), 5)\n",
        "            hourly_rmse = np.round(np.sqrt(mean_squared_error(y_test_hourly, y_pred_hourly)), 5)\n",
        "            daily_mae = np.round(mean_absolute_error(y_test_daily, y_pred_daily), 5)\n",
        "            daily_rmse = np.round(np.sqrt(mean_squared_error(y_test_daily, y_pred_daily)), 5)\n",
        "\n",
        "            print(f\"Vehicle Type: {vehicle_type.replace('Vehicle Type_', '')}\")\n",
        "            print(\"Hourly Rate Model - MAE:\", hourly_mae)\n",
        "            print(\"Hourly Rate Model - RMSE:\", hourly_rmse)\n",
        "            print(\"Daily Rate Model - MAE:\", daily_mae)\n",
        "            print(\"Daily Rate Model - RMSE:\", daily_rmse)\n",
        "            print(\"\\n\")\n",
        "\n",
        "        elif 'Family' in vehicle_type:\n",
        "            # XGBRegressor for hourly rate, DecisionTreeRegressor for daily rate\n",
        "            model_hourly = XGBRegressor()\n",
        "            model_hourly.fit(X_train, y_train_hourly)\n",
        "            joblib.dump(model_hourly, MODEL_PATH + f'{vehicle_type.replace(\"Vehicle Type_\", \"\")}_xgb_hourly_rate_model.pkl')\n",
        "\n",
        "            model_daily = DecisionTreeRegressor()\n",
        "            model_daily.fit(X_train, y_train_daily)\n",
        "            joblib.dump(model_daily, MODEL_PATH + f'{vehicle_type.replace(\"Vehicle Type_\", \"\")}_dt_daily_rate_model.pkl')\n",
        "\n",
        "            # Evaluate models\n",
        "            y_pred_hourly = model_hourly.predict(X_test)\n",
        "            y_pred_daily = model_daily.predict(X_test)\n",
        "\n",
        "            # hourly and daily mae and rmse\n",
        "            hourly_mae = np.round(mean_absolute_error(y_test_hourly, y_pred_hourly), 5)\n",
        "            hourly_rmse = np.round(np.sqrt(mean_squared_error(y_test_hourly, y_pred_hourly)), 5)\n",
        "            daily_mae = np.round(mean_absolute_error(y_test_daily, y_pred_daily), 5)\n",
        "            daily_rmse = np.round(np.sqrt(mean_squared_error(y_test_daily, y_pred_daily)), 5)\n",
        "\n",
        "            print(f\"Vehicle Type: {vehicle_type.replace('Vehicle Type_', '')}\")\n",
        "            print(\"Hourly Rate Model - MAE:\", hourly_mae)\n",
        "            print(\"Hourly Rate Model - RMSE:\", hourly_rmse)\n",
        "            print(\"Daily Rate Model - MAE:\", daily_mae)\n",
        "            print(\"Daily Rate Model - RMSE:\", daily_rmse)\n",
        "            print(\"\\n\")\n",
        "\n",
        "        # BinaryEncoder inverse transform the location columns\n",
        "        location_columns = [col for col in X_test_copy.columns if col.startswith('location_')]\n",
        "        X_test_copy['location'] = binary_encoder.inverse_transform(X_test_copy[location_columns])['location']\n",
        "\n",
        "        # Add predictions to the dataframe\n",
        "        temp_df = pd.DataFrame({\n",
        "            'vehicle_type': vehicle_type.replace('Vehicle Type_', ''),\n",
        "            'location': X_test_copy['location'],\n",
        "            'booking_billed_start': X_test_copy['booking_billed_start'],\n",
        "            'booking_created_at_hour': X_test_copy['booking_created_at_hour'],\n",
        "            'booking_id': X_test_copy['booking_id'],\n",
        "            'booking_mileage': X_test_copy['booking_mileage'],\n",
        "            'booking_rates_hours': X_test_copy['booking_rates_hours'],\n",
        "            'booking_rates_24hours': X_test_copy['booking_rates_24hours'],\n",
        "            'per_mile': X_test_copy['per_mile'],\n",
        "            'booking_actual_cost_distance': X_test_copy['booking_actual_cost_distance'],\n",
        "            'booking_actual_cost_time': X_test_copy['booking_actual_cost_time'],\n",
        "            'booking_actual_cost_total': X_test_copy['booking_actual_cost_total'],\n",
        "            'actual_hourly': y_test_hourly.tolist(),\n",
        "            'predicted_hourly': y_pred_hourly.flatten().tolist(),\n",
        "            'actual_daily': y_test_daily.tolist(),\n",
        "            'predicted_daily': y_pred_daily.flatten().tolist()\n",
        "        })\n",
        "\n",
        "        predictions_df = pd.concat([predictions_df, temp_df], axis=0, ignore_index=True)\n",
        "\n",
        "        # hourly and daily mae and rmse\n",
        "        results[vehicle_type.replace('Vehicle Type_', '')] = {\n",
        "            'hourly_rate': {\n",
        "                'MAE': hourly_mae,\n",
        "                'RMSE': hourly_rmse\n",
        "            },\n",
        "            'daily_rate': {\n",
        "                'MAE': daily_mae,\n",
        "                'RMSE': daily_rmse\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return results, predictions_df"
      ],
      "metadata": {
        "id": "ejqTswsWoaN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results, predictions_df = train_evaluate_models(df)"
      ],
      "metadata": {
        "id": "hOzF0jL_oaKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "8s6yG3v6Yllo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.head()"
      ],
      "metadata": {
        "id": "c3eIYSbncnxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Demand Factor"
      ],
      "metadata": {
        "id": "H0DlOdEpLp2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Popular Location Demand Factor"
      ],
      "metadata": {
        "id": "IzOGfRKQJ5oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for popular location based demand factor\n",
        "def popular_location_demand_factor(historical_data, location):\n",
        "    # Calculate the number of bookings for each location\n",
        "    demand_location = historical_data.groupby('location').size()\n",
        "    demand_location = pd.DataFrame(demand_location, columns=['bookings'])\n",
        "\n",
        "    if location not in demand_location.index:\n",
        "        return 0\n",
        "\n",
        "    # Identify demanded locations\n",
        "    threshold = np.int32(np.round(np.percentile(demand_location['bookings'], 75)))  # Example threshold for peak hours\n",
        "    demand_locations = demand_location[demand_location['bookings'] >= threshold].index.tolist()\n",
        "    max_count = demand_location['bookings'].max()\n",
        "    location_demand = np.round((demand_location['bookings'] / max_count).mean(), 2)\n",
        "\n",
        "    # If location is in demand_locations then location has higher demand than other locations.\n",
        "    if location in demand_locations:\n",
        "        return location_demand\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "rDestgC3lg4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Peak Hour Demand Factor"
      ],
      "metadata": {
        "id": "DFD0IopsKBAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for peak hour demand factor\n",
        "def peak_hour_demand_factor(historical_data, location, hour):\n",
        "    # select subset of data based on location\n",
        "    location_data = historical_data[historical_data['location'] == location]\n",
        "\n",
        "    if location_data.empty:\n",
        "        return 0\n",
        "\n",
        "    # seperate hourly bookings with location\n",
        "    # Aggregate bookings by hour\n",
        "    hourly_bookings = location_data.groupby('booking_created_at_hour').size()\n",
        "    hourly_bookings = pd.DataFrame(hourly_bookings, columns=['bookings'])\n",
        "\n",
        "    # Identify peak hours\n",
        "    threshold = np.int32(np.round(np.percentile(hourly_bookings['bookings'], 75)))  # Example threshold for peak hours\n",
        "    peak_hours = hourly_bookings[hourly_bookings['bookings'] >= threshold].index.tolist()\n",
        "    max_count = hourly_bookings['bookings'].max()\n",
        "    peak_hour_demand = np.round((hourly_bookings['bookings'] / max_count).mean(), 2)\n",
        "\n",
        "    # If hour is in peak_hours then it has higher demand than other hours.\n",
        "    if hour in peak_hours:\n",
        "        return peak_hour_demand\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "HV_veH7kodWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Overall Demand Factor"
      ],
      "metadata": {
        "id": "_AJQM0GAKDhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for demand factor\n",
        "def demand_factor(historical_data, location, hour):\n",
        "    hour_demand_factor = peak_hour_demand_factor(historical_data, location, hour)\n",
        "    location_demand_factor = popular_location_demand_factor(historical_data, location)\n",
        "    final_demand_factor = np.round(np.mean([hour_demand_factor, location_demand_factor]), 5)\n",
        "    return final_demand_factor"
      ],
      "metadata": {
        "id": "ZuxMI4RalfDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort dataset based on booking_billed_start\n",
        "predictions_df.sort_values(by='booking_billed_start', inplace=True)"
      ],
      "metadata": {
        "id": "eO7frvDMixB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply demand factor to predicted dataframe\n",
        "predictions_df['demand_factor'] = predictions_df.apply(lambda row: demand_factor(df_transformed, row['location'], row['booking_created_at_hour']), axis=1)\n",
        "\n",
        "# Use dynamic pricing formula for hourly_rate and daily_rate\n",
        "predictions_df['adjusted_hourly'] = predictions_df['predicted_hourly'] + (predictions_df['demand_factor'] * predictions_df['predicted_hourly'])\n",
        "predictions_df['adjusted_daily'] = predictions_df['predicted_daily'] + (predictions_df['demand_factor'] * predictions_df['predicted_daily'])"
      ],
      "metadata": {
        "id": "sSTDm1GMj1pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.head()"
      ],
      "metadata": {
        "id": "oxT6N8UNkfW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Compare Revenues"
      ],
      "metadata": {
        "id": "kbwqpiHrk49I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Inverse Log Transformation"
      ],
      "metadata": {
        "id": "Gu7mLcKpNGtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['booking_mileage', 'booking_actual_cost_distance',\n",
        "                      'booking_actual_cost_time', 'booking_actual_cost_total']\n",
        "\n",
        "# Apply inverse log transformation to numerical features\n",
        "for feature in numerical_features:\n",
        "    predictions_df[feature] = np.expm1(predictions_df[feature])"
      ],
      "metadata": {
        "id": "o2t91_MgNHDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.head()"
      ],
      "metadata": {
        "id": "yo2lLhkupuZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Actual Revenue"
      ],
      "metadata": {
        "id": "qV08Ssb7uqRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate revenue for the actual values\n",
        "predictions_df['actual_revenue'] = predictions_df['booking_actual_cost_total']"
      ],
      "metadata": {
        "id": "39HHm8dkk4oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Dynamically Adjusted Revenue"
      ],
      "metadata": {
        "id": "ArvF9cerutJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate booking adjusted cost distance\n",
        "predictions_df['booking_adjusted_cost_distance'] = predictions_df['per_mile'] * predictions_df['booking_mileage']\n",
        "\n",
        "# Calculate booking adjusted cost time\n",
        "predictions_df['booking_adjusted_cost_time'] = (predictions_df['booking_rates_hours'] * predictions_df['adjusted_hourly']) + (predictions_df['booking_rates_24hours'] * predictions_df['adjusted_daily'])"
      ],
      "metadata": {
        "id": "bvdAl2cnux6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate revenue for the adjusted values\n",
        "predictions_df['adjusted_revenue'] = predictions_df['booking_adjusted_cost_distance'] + predictions_df['booking_adjusted_cost_time']"
      ],
      "metadata": {
        "id": "pCezfmeiv8G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df.head()"
      ],
      "metadata": {
        "id": "DzbQ2sOo1--0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Plot Comparison between revenues"
      ],
      "metadata": {
        "id": "AW5wVl3kAFlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.1. Comparison by location"
      ],
      "metadata": {
        "id": "vOYsdFQs0shp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to format the y-axis labels\n",
        "def format_y_axis(value, tick_number):\n",
        "    return f'{value:,.0f}'"
      ],
      "metadata": {
        "id": "ikjOKgRfwabz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs adjusted revenue\n",
        "def plot_revenue_comparison(df):\n",
        "    grouped_df = df.groupby(['location'])[['actual_revenue', 'adjusted_revenue']].sum().reset_index()\n",
        "\n",
        "    # Plotting the data\n",
        "    plt.figure(figsize=(18, 7))\n",
        "\n",
        "    locations = grouped_df['location']\n",
        "    actual_revenue = grouped_df['actual_revenue']\n",
        "    adjusted_revenue = grouped_df['adjusted_revenue']\n",
        "\n",
        "    bar_width = 0.35\n",
        "    index = range(len(locations))\n",
        "\n",
        "    # Creating bars for actual and adjusted revenue\n",
        "    plt.bar(index, actual_revenue, bar_width, label='Actual Revenue')\n",
        "    plt.bar([i + bar_width for i in index], adjusted_revenue, bar_width, label='Adjusted Revenue')\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.xlabel('Location')\n",
        "    plt.ylabel('Revenue')\n",
        "    plt.title('Comparison of Actual and Adjusted Revenue by Location')\n",
        "    plt.xticks([i + bar_width / 2 for i in index], locations)\n",
        "    plt.legend()\n",
        "\n",
        "    # Formatting the y-axis\n",
        "    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_y_axis))\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4BmXlSBLBfpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_revenue_comparison(predictions_df)"
      ],
      "metadata": {
        "id": "1t3tMAuAqy8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4.2. Overall Comparison"
      ],
      "metadata": {
        "id": "5o1z76Wj00lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs adjusted revenue\n",
        "def plot_revenue_comparison_overall(df):\n",
        "    # sum of actual and adjusted revenues\n",
        "    actual_adjusted_revenues = predictions_df[['actual_revenue', 'adjusted_revenue']].sum()\n",
        "\n",
        "    # Plotting the data\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    bars = plt.bar(['Actual Revenue', 'Adjusted Revenue'], actual_adjusted_revenues, color=['blue', 'orange'])\n",
        "\n",
        "    # Adding labels and title\n",
        "    plt.xlabel('Type of Revenues')\n",
        "    plt.ylabel('Revenue')\n",
        "    plt.title('Comparison of Actual and Adjusted Revenues')\n",
        "    plt.xticks([0, 1], ['Actual Revenue', 'Adjusted Revenue'])\n",
        "\n",
        "    # Adding legends\n",
        "    plt.legend(bars, ['Actual Revenue', 'Adjusted Revenue'])\n",
        "\n",
        "    # Formatting the y-axis\n",
        "    plt.gca().yaxis.set_major_formatter(FuncFormatter(format_y_axis))\n",
        "    plt.xticks(rotation=0)\n",
        "\n",
        "    # add difference of the actual and adjusted revenue on the top of the plot\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'£{yval:,.0f}', ha='center', va='bottom', fontsize=15)\n",
        "\n",
        "    # plot difference line\n",
        "    plt.plot([0, 1], [actual_adjusted_revenues['actual_revenue'], actual_adjusted_revenues['adjusted_revenue']], color='red', linestyle='--', linewidth=2, marker='o')\n",
        "\n",
        "    # plot the difference\n",
        "    actual_adjusted_revenues['difference'] = actual_adjusted_revenues['adjusted_revenue'] - actual_adjusted_revenues['actual_revenue']\n",
        "    actual_adjusted_revenues['difference_pct'] = np.round(((actual_adjusted_revenues['adjusted_revenue'] - actual_adjusted_revenues['actual_revenue'])/actual_adjusted_revenues['actual_revenue'])*100, 2)\n",
        "    plt.text(0.5, actual_adjusted_revenues['actual_revenue'] + (actual_adjusted_revenues['actual_revenue']*0.15), f'£{actual_adjusted_revenues[\"difference\"]:,.0f} ({actual_adjusted_revenues[\"difference_pct\"]} %)', ha='center', va='bottom', color='red', fontsize=17)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hyRBO5-v03wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_revenue_comparison_overall(predictions_df)"
      ],
      "metadata": {
        "id": "ChUyqUxU3LbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "O8didB6VNryK"
      }
    }
  ]
}